{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_agents = [\n",
    "#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "#     'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "#     'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "#     # Add more User-Agents if needed\n",
    "# ]\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': \"Mozilla/5.0 (platform; rv:gecko-version) Gecko/gecko-trail Firefox/132.0.1-1\"\n",
    "}\n",
    "\n",
    "def get_sitemaps() -> list:\n",
    "    # URL\n",
    "    sitemap = \"https://www.metacritic.com/games.xml\"\n",
    "    sm_response = requests.get(sitemap, headers=headers)\n",
    "    sm_soup = BeautifulSoup(sm_response.text, 'xml')\n",
    "    # time.sleep(1)\n",
    "    return [sm.get_text(strip=True) for sm in sm_soup.find_all('sitemap')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(sitemap: str):\n",
    "    try:\n",
    "        urls_response = requests.get(sitemap, headers=headers, timeout=3)\n",
    "    except Exception:\n",
    "        return sitemap\n",
    "    \n",
    "    urls_soup = BeautifulSoup(urls_response.text, 'xml')\n",
    "    # time.sleep(1)\n",
    "    urls = urls_soup.find_all('loc')\n",
    "    if not urls:\n",
    "        raise ValueError('No URLs found in sitemap')\n",
    "    \n",
    "    return [url.get_text(strip=True) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url: str) -> pd.DataFrame:\n",
    "    # Data Dictionary\n",
    "    data = {\n",
    "        'Game Title': [],\n",
    "        'Game Genre': [],\n",
    "        'Pricing': [],\n",
    "        'Publisher': [],\n",
    "        'Release Date': [],\n",
    "        'Platform': [],\n",
    "        'Rating': [],\n",
    "        'Number of Ratings': []\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # print(response.status_code)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # time.sleep(1)\n",
    "    if not soup:\n",
    "        raise ValueError('No data found in URL')\n",
    "    if soup.find('div', class_='c-error404'):\n",
    "        raise ValueError('404 Error')\n",
    "    \n",
    "    # Rating\n",
    "    rating = soup.find('div', class_='c-productScoreInfo u-clearfix')\n",
    "    if rating:\n",
    "        text = rating.find('div', class_='c-siteReviewScore_background c-siteReviewScore_background-user').get_text(strip=True)\n",
    "        if text == 'tbd':\n",
    "            raise ValueError('tbd')\n",
    "        else:\n",
    "            data['Rating'].append(text)\n",
    "            number_of_ratings = rating.find('span', class_='c-productScoreInfo_reviewsTotal u-block').get_text(strip=True).replace('Based on ', '').replace(' User Ratings', '').replace(',', '')\n",
    "            data['Number of Ratings'].append(int(number_of_ratings))\n",
    "    else:\n",
    "        data['Rating'].append(np.nan)\n",
    "    \n",
    "    # Title\n",
    "    title = soup.find(\"div\", class_=\"c-productHero_title g-inner-spacing-bottom-medium g-outer-spacing-top-medium\")\n",
    "    if title:\n",
    "        data['Game Title'].append(title.get_text(strip=True))\n",
    "    else:\n",
    "        data['Game Title'].append(np.nan)\n",
    "    \n",
    "    # Genre\n",
    "    genre = soup.find('li', class_='c-genreList_item')\n",
    "    if genre:\n",
    "        data['Game Genre'].append(genre.get_text(strip=True))\n",
    "    else:\n",
    "        data['Game Genre'].append(np.nan)\n",
    "    \n",
    "    # Publisher\n",
    "    publisher = soup.find('div', class_='c-gameDetails_Distributor u-flexbox u-flexbox-row')\n",
    "    if publisher:\n",
    "        data['Publisher'].append(publisher.get_text(strip=True).replace('Publisher:', ''))\n",
    "    else:\n",
    "        data['Publisher'].append(np.nan)\n",
    "    \n",
    "    # Release Date\n",
    "    release_date = soup.find('div', class_='c-gameDetails_ReleaseDate u-flexbox u-flexbox-row')\n",
    "    if release_date:\n",
    "        data['Release Date'].append(release_date.get_text(strip=True).replace('Initial Release Date:', ''))\n",
    "    else:\n",
    "        data['Release Date'].append(np.nan)\n",
    "    \n",
    "    # Platform\n",
    "    platform = soup.find('div', class_='c-gameDetails_Platforms u-flexbox u-flexbox-row')\n",
    "    if platform:\n",
    "        platforms = []\n",
    "        for p in platform.find_all('li', class_='c-gameDetails_listItem g-color-gray70 u-inline-block'):\n",
    "            platforms.append(p.get_text(strip=True))\n",
    "        # Join the platforms as a comma-separated string\n",
    "        platforms_text = \", \".join(platforms)\n",
    "        data['Platform'].append(platforms_text)\n",
    "    else:\n",
    "        data['Platform'].append(np.nan)\n",
    "\n",
    "    # Price\n",
    "    price = soup.find('div', {'data-cy': 'w2w-purchase-button'})\n",
    "    if price:\n",
    "        data['Pricing'].append(price.find('span', class_='g-outer-spacing-left-small').get_text(strip=True).replace('(', '').replace(')', ''))\n",
    "    else:\n",
    "        data['Pricing'].append(np.nan)\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_sitemap = get_sitemaps()\n",
    "total_sitemaps = len(main_sitemap)\n",
    "\n",
    "def get_game_links(sitemaps, retry=0):\n",
    "    \n",
    "    if len(sitemaps) == 0:\n",
    "        return\n",
    "\n",
    "    missed_urls = []\n",
    "    success_urls = []\n",
    "\n",
    "    for i,sitemap in enumerate(sitemaps):\n",
    "        \n",
    "        print(f\"\\rFound {len(success_urls)}; searching at {i}/{len(sitemaps)}\", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            urls = get_urls(sitemap)\n",
    "            success_urls += urls\n",
    "        except Exception:\n",
    "            missed_urls.append(sitemap)\n",
    "            continue\n",
    "\n",
    "    if success_urls:\n",
    "        with open(f\"{retry}_metacritic.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(success_urls))\n",
    "\n",
    "    print(f\"\\n{len(success_urls)} URLs found, {len(missed_urls)} sitemaps inaccessible\")\n",
    "    \n",
    "    get_game_links(missed_urls, retry + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 248999; searching at 249/250\n",
      "249724 URLs found, 0 sitemaps inaccessible\n"
     ]
    }
   ],
   "source": [
    "get_game_links(main_sitemap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
