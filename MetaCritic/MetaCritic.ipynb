{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': \"Mozilla/5.0 (platform; rv:gecko-version) Gecko/gecko-trail Firefox/132.0.1-1\"\n",
    "}\n",
    "\n",
    "def get_sitemaps() -> list:\n",
    "    # URL\n",
    "    sitemap = \"https://www.metacritic.com/games.xml\"\n",
    "    sm_response = requests.get(sitemap, headers=headers)\n",
    "    sm_soup = BeautifulSoup(sm_response.text, 'xml')\n",
    "    # time.sleep(1)\n",
    "    return [sm.get_text(strip=True) for sm in sm_soup.find_all('sitemap')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(sitemap: str):\n",
    "    try:\n",
    "        urls_response = requests.get(sitemap, headers=headers, timeout=3)\n",
    "    except Exception:\n",
    "        return sitemap\n",
    "    \n",
    "    urls_soup = BeautifulSoup(urls_response.text, 'xml')\n",
    "    # time.sleep(1)\n",
    "    urls = urls_soup.find_all('loc')\n",
    "    if not urls:\n",
    "        raise ValueError('No URLs found in sitemap')\n",
    "    \n",
    "    return [url.get_text(strip=True) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(url):\n",
    "    data = {\n",
    "        'Game Title': [],\n",
    "        'Game Genre': [],\n",
    "        'Pricing': 'n/a',\n",
    "        'Developer': [],\n",
    "        'Release Date': [],\n",
    "        'Platform': [],\n",
    "        'Rating': [],\n",
    "        'Number of Ratings': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "    except Exception:\n",
    "        raise Exception(e)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "    info = soup.find('div', class_='c-productHero_score-container u-flexbox u-flexbox-column g-bg-white')\n",
    "    \n",
    "    # rating\n",
    "    ratings = info.find_all('div', class_='c-productScoreInfo_scoreContent')\n",
    "    \n",
    "    score = ratings[1].find('div', class_='c-productScoreInfo_scoreNumber').get_text(strip=True)\n",
    "    \n",
    "    if score == 'tbd':\n",
    "        raise ValueError('tbd')\n",
    "\n",
    "    data['Rating'] = score\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    num_rating_text = ratings[1].find('span', class_='c-productScoreInfo_reviewsTotal').get_text(strip=True)\n",
    "    num_ratings = re.search(r'(\\d[\\d,]*)', num_rating_text).group(1)\n",
    "    \n",
    "    data['Number of Ratings'] = num_ratings.replace(',', '')\n",
    "    \n",
    "    # title\n",
    "    data['Game Title'] = soup.find('div', class_='c-productHero_title').get_text(strip=True)\n",
    "    \n",
    "    # genre\n",
    "    genres = soup.find('ul', class_='c-genreList').select('li')\n",
    "    data['Game Genre'] = \", \".join([genre.get_text(strip=True) for genre in genres])\n",
    "    \n",
    "    \n",
    "    # developer\n",
    "    devs = soup.find('div', class_='c-gameDetails_Developer').select('.c-gameDetails_listItem')\n",
    "    data['Developer'] = \", \".join([dev.get_text(strip=True) for dev in devs])\n",
    "    \n",
    "    # release date\n",
    "    release_date = soup.find('div', class_='c-gameDetails_ReleaseDate').select(\"span\")[1].get_text(strip=True)\n",
    "    data['Release Date'] = release_date\n",
    "    \n",
    "    # platforms\n",
    "    platforms = [li.get_text(strip=True) for li in soup.select(\".c-gameDetails_Platforms ul li\")]\n",
    "    data['Platform'] = \", \".join(platforms)\n",
    "    \n",
    "    return pd.DataFrame([data])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_sitemap = get_sitemaps()\n",
    "total_sitemaps = len(main_sitemap)\n",
    "\n",
    "def get_game_links(sitemaps, retry=0):\n",
    "    \n",
    "    if len(sitemaps) == 0:\n",
    "        return\n",
    "\n",
    "    missed_urls = []\n",
    "    success_urls = []\n",
    "\n",
    "    for i,sitemap in enumerate(sitemaps):\n",
    "        \n",
    "        print(f\"\\rFound {len(success_urls)}; searching at {i}/{len(sitemaps)}\", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            urls = get_urls(sitemap)\n",
    "            success_urls += urls\n",
    "        except Exception:\n",
    "            missed_urls.append(sitemap)\n",
    "            continue\n",
    "\n",
    "    if success_urls:\n",
    "        with open(f\"{retry}_metacritic.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(success_urls))\n",
    "\n",
    "    print(f\"\\n{len(success_urls)} URLs found, {len(missed_urls)} sitemaps inaccessible\")\n",
    "    \n",
    "    get_game_links(missed_urls, retry + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_game_links(main_sitemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(urls: list, part: str):\n",
    "    size = len(urls)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "    df_list = []\n",
    "\n",
    "    for i,url in enumerate(urls):\n",
    "        try:\n",
    "            data = scrape_data(url)\n",
    "            df_list.append(data)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        print(f\"\\r[{int(((i+1)/size)*100)}%] {i}/{size}, scrapped {len(df_list)}\", end=\"\")\n",
    "            \n",
    "            \n",
    "    main_df = pd.concat(df_list, ignore_index=True)\n",
    "    main_df.to_csv(f\"{part}_metacritic_data.csv\", index=False)\n",
    "    \n",
    "    print(main_df.shape)\n",
    "    print(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metacritic_urls.txt\", \"r\") as file:\n",
    "    urls = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chạy từng part\n",
    "> **NẾU CHIA THÀNH NHIỀU RANGE ĐỂ CHẠY THÌ ĐỪNG CHO HÀM `run()` VÀO VÒNG LẶP MÀ HÃY CHẠY RIÊNG MỖI CELL**\n",
    "### VD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(urls[:50000], \"part1\") # chạy từ 50000 link đầu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(urls[50000:100000], \"part2\") # 50000 link tiếp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(urls[100000:150000], \"part3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(urls[150000:200000], \"part4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiếp tục\n",
    "# run(urls[200000:], \"part5\")\n",
    "# chạy phần còn lại"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
