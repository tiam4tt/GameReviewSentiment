{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 16585 samples\n",
      "Validation set size: 2073 samples\n",
      "Test set size: 2074 samples\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Create 'is_paid' feature: 0 for free, 1 for paid\n",
    "data['is_paid'] = data['Pricing'].apply(lambda x: 0 if x == 0.0 else 1)\n",
    "\n",
    "# Extract 'Month' from 'Release Date'\n",
    "data['Release Date'] = pd.to_datetime(data['Release Date'], errors='coerce')\n",
    "data['Release Month'] = data['Release Date'].dt.month\n",
    "\n",
    "# Filter only free games (is_paid == 0)\n",
    "data_free = data[data['is_paid'] == 0]\n",
    "\n",
    "# Select features and target, excluding 'Pricing'\n",
    "features = ['Game Genre', 'Developer', 'Release Month']\n",
    "target = 'Rating'\n",
    "\n",
    "X = data_free[features]\n",
    "y = data_free[target]\n",
    "\n",
    "# Reset the index of y to align with X\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['Game Genre', 'Developer']\n",
    "numerical_features = ['Release Month']\n",
    "\n",
    "# Preprocessing pipelines for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert the preprocessed features to a DataFrame\n",
    "encoded_cat_features = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "encoded_num_features = numerical_features\n",
    "all_features = list(encoded_cat_features) + encoded_num_features\n",
    "\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed.toarray(), columns=all_features)\n",
    "\n",
    "# Reset the index of X_preprocessed_df to align with y\n",
    "X_preprocessed_df = X_preprocessed_df.reset_index(drop=True)\n",
    "\n",
    "# Split data into training and temporary sets (80% train, 20% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_preprocessed_df, y, test_size=0.2, random_state=18\n",
    ")\n",
    "\n",
    "# Split temporary set into validation and test sets (50% each of temp -> 10% each of original)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=18\n",
    ")\n",
    "\n",
    "# Display the sizes of the splits\n",
    "print(f'Training set size: {X_train.shape[0]} samples')\n",
    "print(f'Validation set size: {X_valid.shape[0]} samples')\n",
    "print(f'Test set size: {X_test.shape[0]} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Parameter Grids for Regression Models\n",
    "import itertools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Parameter grids for each model\n",
    "param_grids = {\n",
    "    'LinearRegression': {\n",
    "        'fit_intercept': [True, False]\n",
    "    },\n",
    "    'PolynomialRegression': {\n",
    "        'polynomialfeatures__degree': [2, 3, 4],\n",
    "        'polynomialfeatures__include_bias': [False],\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__normalize': [True, False]\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'max_iter': [200, 300, 500]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Hàm cross-validation cho hồi quy\n",
    "def cross_validate_regression(model, X, y, k=5):\n",
    "    fold_size = len(X) // k\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    scores = {'mse': [], 'rmse': [], 'r2': []}\n",
    "    \n",
    "    for fold in range(k):\n",
    "        start = fold * fold_size\n",
    "        end = start + fold_size if fold != k-1 else len(X)\n",
    "        val_indices = indices[start:end]\n",
    "        train_indices = np.concatenate([indices[:start], indices[end:]])\n",
    "        \n",
    "        # Convert X và y thành numpy arrays nếu là pandas DataFrame hoặc Series\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "            y = y.values\n",
    "        \n",
    "        X_train_cv, y_train_cv = X[train_indices], y[train_indices]\n",
    "        X_val_cv, y_val_cv = X[val_indices], y[val_indices]\n",
    "        \n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred = model.predict(X_val_cv)\n",
    "        \n",
    "        mse = mean_squared_error(y_val_cv, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_val_cv, y_pred)\n",
    "        \n",
    "        scores['mse'].append(mse)\n",
    "        scores['rmse'].append(rmse)\n",
    "        scores['r2'].append(r2)\n",
    "        \n",
    "    avg_scores = {metric: np.mean(values) for metric, values in scores.items()}\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Hyperparameter Tuning for Linear Regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Khởi tạo các biến để lưu kết quả\n",
    "best_lr_score = -np.inf\n",
    "best_lr_mse = np.inf\n",
    "best_lr_params = {}\n",
    "lr_results = []\n",
    "\n",
    "# Quá trình tuning tham số LinearRegression\n",
    "for fit_intercept in param_grids['LinearRegression']['fit_intercept']:\n",
    "    for normalize in [True, False]:  # Giờ sử dụng chuẩn hóa với StandardScaler\n",
    "        # Sử dụng pipeline với StandardScaler và LinearRegression\n",
    "        if normalize:\n",
    "            model = make_pipeline(StandardScaler(), LinearRegression(fit_intercept=fit_intercept))\n",
    "        else:\n",
    "            model = LinearRegression(fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Đánh giá mô hình với k-fold cross-validation\n",
    "        scores = cross_validate_regression(model, X_train, y_train, k=5)\n",
    "        \n",
    "        avg_r2 = scores['r2']\n",
    "        avg_mse = scores['mse']\n",
    "        \n",
    "        # Lưu kết quả vào danh sách\n",
    "        lr_results.append({\n",
    "            'fit_intercept': fit_intercept,\n",
    "            'normalize': normalize,\n",
    "            'R2': avg_r2,\n",
    "            'MSE': avg_mse\n",
    "        })\n",
    "        \n",
    "        # Cập nhật tham số tốt nhất\n",
    "        if avg_r2 > best_lr_score and avg_mse < best_lr_mse:\n",
    "            best_lr_score = avg_r2\n",
    "            best_lr_mse = avg_mse\n",
    "            best_lr_params = {\n",
    "                'fit_intercept': fit_intercept,\n",
    "                'normalize': normalize\n",
    "            }\n",
    "\n",
    "# In tất cả các kết quả tuning\n",
    "print(\"\\nTất cả các kết quả tuning tham số LinearRegression:\")\n",
    "for result in lr_results:\n",
    "    print(f\"fit_intercept = {result['fit_intercept']}, normalize = {result['normalize']}, R² = {result['R2']}, MSE = {result['MSE']}\")\n",
    "\n",
    "# In báo cáo quá trình tuning\n",
    "print(\"\\nQuá trình tuning tham số LinearRegression đã hoàn thành!\")\n",
    "print(f\"Tham số tốt nhất: {best_lr_params}\")\n",
    "print(f\"Điểm R² tốt nhất (CV): {best_lr_score}\")\n",
    "print(f\"MSE tốt nhất (CV): {best_lr_mse}\")\n",
    "\n",
    "# Huấn luyện lại mô hình với tham số tốt nhất trên toàn bộ tập huấn luyện\n",
    "if best_lr_params['normalize']:\n",
    "    best_model = make_pipeline(StandardScaler(), LinearRegression(fit_intercept=best_lr_params['fit_intercept']))\n",
    "else:\n",
    "    best_model = LinearRegression(fit_intercept=best_lr_params['fit_intercept'])\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Dự đoán trên tập kiểm tra và tính toán các chỉ số R², MSE, RMSE\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "# In kết quả trên tập kiểm tra\n",
    "print(\"\\nKết quả trên tập kiểm tra với tham số tốt nhất:\")\n",
    "print(f\"Điểm R² trên tập kiểm tra: {test_r2}\")\n",
    "print(f\"MSE trên tập kiểm tra: {test_mse}\")\n",
    "print(f\"RMSE trên tập kiểm tra: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Hyperparameter Tuning for Polynomial Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "best_pr_score = -np.inf\n",
    "best_pr_params = {}\n",
    "pr_results = []\n",
    "\n",
    "for degree, include_bias, fit_intercept, normalize in itertools.product(\n",
    "    param_grids['PolynomialRegression']['polynomialfeatures__degree'],\n",
    "    param_grids['PolynomialRegression']['polynomialfeatures__include_bias'],\n",
    "    param_grids['PolynomialRegression']['linearregression__fit_intercept'],\n",
    "    param_grids['PolynomialRegression']['linearregression__normalize']\n",
    "):\n",
    "    pipeline = Pipeline([\n",
    "        ('polynomialfeatures', PolynomialFeatures(\n",
    "            degree=degree,\n",
    "            include_bias=include_bias\n",
    "        )),\n",
    "        ('linearregression', LinearRegression(\n",
    "            fit_intercept=fit_intercept,\n",
    "            normalize=normalize\n",
    "        ))\n",
    "    ])\n",
    "    scores = cross_validate_regression(pipeline, X_train, y_train, k=5)\n",
    "    avg_r2 = scores['r2']\n",
    "    pr_results.append({\n",
    "        'degree': degree,\n",
    "        'include_bias': include_bias,\n",
    "        'fit_intercept': fit_intercept,\n",
    "        'normalize': normalize,\n",
    "        'R2': avg_r2\n",
    "    })\n",
    "    if avg_r2 > best_pr_score:\n",
    "        best_pr_score = avg_r2\n",
    "        best_pr_params = {\n",
    "            'degree': degree,\n",
    "            'include_bias': include_bias,\n",
    "            'fit_intercept': fit_intercept,\n",
    "            'normalize': normalize\n",
    "        }\n",
    "\n",
    "print('Best PolynomialRegression Params:', best_pr_params)\n",
    "print('Best PolynomialRegression CV R2:', best_pr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Hyperparameter Tuning for Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "best_rf_score = -np.inf\n",
    "best_rf_params = {}\n",
    "rf_results = []\n",
    "\n",
    "for n_estimators, max_depth, min_samples_split in itertools.product(\n",
    "    param_grids['RandomForestRegressor']['n_estimators'],\n",
    "    param_grids['RandomForestRegressor']['max_depth'],\n",
    "    param_grids['RandomForestRegressor']['min_samples_split']\n",
    "):\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    scores = cross_validate_regression(model, X_train, y_train, k=5)\n",
    "    avg_r2 = scores['r2']\n",
    "    rf_results.append({\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'R2': avg_r2\n",
    "    })\n",
    "    if avg_r2 > best_rf_score:\n",
    "        best_rf_score = avg_r2\n",
    "        best_rf_params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split\n",
    "        }\n",
    "\n",
    "print('Best RandomForestRegressor Params:', best_rf_params)\n",
    "print('Best RandomForestRegressor CV R2:', best_rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Hyperparameter Tuning for MLP Regressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "best_mlp_score = -np.inf\n",
    "best_mlp_params = {}\n",
    "mlp_results = []\n",
    "\n",
    "for hidden_layer_sizes, activation, solver, alpha, learning_rate in itertools.product(\n",
    "    param_grids['MLPRegressor']['hidden_layer_sizes'],\n",
    "    param_grids['MLPRegressor']['activation'],\n",
    "    param_grids['MLPRegressor']['solver'],\n",
    "    param_grids['MLPRegressor']['alpha'],\n",
    "    param_grids['MLPRegressor']['learning_rate']\n",
    "):\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_validate_regression(model, X_train, y_train, k=5)\n",
    "        avg_r2 = scores['r2']\n",
    "        mlp_results.append({\n",
    "            'hidden_layer_sizes': hidden_layer_sizes,\n",
    "            'activation': activation,\n",
    "            'solver': solver,\n",
    "            'alpha': alpha,\n",
    "            'learning_rate': learning_rate,\n",
    "            'R2': avg_r2\n",
    "        })\n",
    "        if avg_r2 > best_mlp_score:\n",
    "            best_mlp_score = avg_r2\n",
    "            best_mlp_params = {\n",
    "                'hidden_layer_sizes': hidden_layer_sizes,\n",
    "                'activation': activation,\n",
    "                'solver': solver,\n",
    "                'alpha': alpha,\n",
    "                'learning_rate': learning_rate\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f'Error with params {hidden_layer_sizes, activation, solver, alpha, learning_rate}: {e}')\n",
    "        continue\n",
    "\n",
    "print('Best MLPRegressor Params:', best_mlp_params)\n",
    "print('Best MLPRegressor CV R2:', best_mlp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train Best Models on Training Set and Evaluate on Test Set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Linear Regression\n",
    "best_lr = LinearRegression(\n",
    "    fit_intercept=best_lr_params['fit_intercept'],\n",
    "    normalize=best_lr_params['normalize']\n",
    ")\n",
    "best_lr.fit(X_train, y_train)\n",
    "lr_pred = best_lr.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_r2 = r2_score(y_test, lr_pred)\n",
    "\n",
    "print('LinearRegression Test MSE:', lr_mse)\n",
    "print('LinearRegression Test RMSE:', lr_rmse)\n",
    "print('LinearRegression Test R2:', lr_r2)\n",
    "\n",
    "# Polynomial Regression\n",
    "best_pr = Pipeline([\n",
    "    ('polynomialfeatures', PolynomialFeatures(\n",
    "        degree=best_pr_params['degree'],\n",
    "        include_bias=best_pr_params['include_bias']\n",
    "    )),\n",
    "    ('linearregression', LinearRegression(\n",
    "        fit_intercept=best_pr_params['fit_intercept'],\n",
    "        normalize=best_pr_params['normalize']\n",
    "    ))\n",
    "])\n",
    "best_pr.fit(X_train, y_train)\n",
    "pr_pred = best_pr.predict(X_test)\n",
    "pr_mse = mean_squared_error(y_test, pr_pred)\n",
    "pr_rmse = np.sqrt(pr_mse)\n",
    "pr_r2 = r2_score(y_test, pr_pred)\n",
    "\n",
    "print('PolynomialRegression Test MSE:', pr_mse)\n",
    "print('PolynomialRegression Test RMSE:', pr_rmse)\n",
    "print('PolynomialRegression Test R2:', pr_r2)\n",
    "\n",
    "# Random Forest Regressor\n",
    "best_rf = RandomForestRegressor(\n",
    "    n_estimators=best_rf_params['n_estimators'],\n",
    "    max_depth=best_rf_params['max_depth'],\n",
    "    min_samples_split=best_rf_params['min_samples_split'],\n",
    "    random_state=42\n",
    ")\n",
    "best_rf.fit(X_train, y_train)\n",
    "rf_pred = best_rf.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "\n",
    "print('RandomForestRegressor Test MSE:', rf_mse)\n",
    "print('RandomForestRegressor Test RMSE:', rf_rmse)\n",
    "print('RandomForestRegressor Test R2:', rf_r2)\n",
    "\n",
    "# MLP Regressor\n",
    "best_mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=best_mlp_params['hidden_layer_sizes'],\n",
    "    activation=best_mlp_params['activation'],\n",
    "    solver=best_mlp_params['solver'],\n",
    "    alpha=best_mlp_params['alpha'],\n",
    "    learning_rate=best_mlp_params['learning_rate'],\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "best_mlp.fit(X_train, y_train)\n",
    "mlp_pred = best_mlp.predict(X_test)\n",
    "mlp_mse = mean_squared_error(y_test, mlp_pred)\n",
    "mlp_rmse = np.sqrt(mlp_mse)\n",
    "mlp_r2 = r2_score(y_test, mlp_pred)\n",
    "\n",
    "print('MLPRegressor Test MSE:', mlp_mse)\n",
    "print('MLPRegressor Test RMSE:', mlp_rmse)\n",
    "print('MLPRegressor Test R2:', mlp_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Compare Model Performances\n",
    "import pandas as pd\n",
    "\n",
    "performance = pd.DataFrame({\n",
    "    'Model': ['LinearRegression', 'PolynomialRegression', 'RandomForestRegressor', 'MLPRegressor'],\n",
    "    'MSE': [lr_mse, pr_mse, rf_mse, mlp_mse],\n",
    "    'RMSE': [lr_rmse, pr_rmse, rf_rmse, mlp_rmse],\n",
    "    'R2': [lr_r2, pr_r2, rf_r2, mlp_r2]\n",
    "})\n",
    "\n",
    "print('Model Comparison:')\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Report Fine-Tuning Process and Model Performances\n",
    "\n",
    "print(\"=== Hyperparameter Tuning Results ===\\n\")\n",
    "\n",
    "print(\"1. **Linear Regression**\")\n",
    "print(f\"   - Best Parameters: {best_lr_params}\")\n",
    "print(f\"   - Best CV R² Score: {best_lr_score}\\n\")\n",
    "\n",
    "print(\"2. **Polynomial Regression**\")\n",
    "print(f\"   - Best Parameters: {best_pr_params}\")\n",
    "print(f\"   - Best CV R² Score: {best_pr_score}\\n\")\n",
    "\n",
    "print(\"3. **Random Forest Regressor**\")\n",
    "print(f\"   - Best Parameters: {best_rf_params}\")\n",
    "print(f\"   - Best CV R² Score: {best_rf_score}\\n\")\n",
    "\n",
    "print(\"4. **MLP Regressor**\")\n",
    "print(f\"   - Best Parameters: {best_mlp_params}\")\n",
    "print(f\"   - Best CV R² Score: {best_mlp_score}\\n\")\n",
    "\n",
    "print(\"=== Model Performance on Test Set ===\\n\")\n",
    "\n",
    "performance = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Polynomial Regression', 'Random Forest Regressor', 'MLP Regressor'],\n",
    "    'MSE': [lr_mse, pr_mse, rf_mse, mlp_mse],\n",
    "    'RMSE': [lr_rmse, pr_rmse, rf_rmse, mlp_rmse],\n",
    "    'R²': [lr_r2, pr_r2, rf_r2, mlp_r2]\n",
    "})\n",
    "\n",
    "print(performance.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
