{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 16585 samples\n",
      "Validation set size: 2073 samples\n",
      "Test set size: 2074 samples\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Create 'is_paid' feature: 0 for free, 1 for paid\n",
    "data['is_paid'] = data['Pricing'].apply(lambda x: 0 if x == 0.0 else 1)\n",
    "\n",
    "# Extract 'Month' from 'Release Date'\n",
    "data['Release Date'] = pd.to_datetime(data['Release Date'], errors='coerce')\n",
    "data['Release Month'] = data['Release Date'].dt.month\n",
    "\n",
    "# Filter only free games (is_paid == 0)\n",
    "data_free = data[data['is_paid'] == 0]\n",
    "\n",
    "# Select features and target, excluding 'Pricing'\n",
    "features = ['Game Genre', 'Developer', 'Release Month']\n",
    "target = 'Rating'\n",
    "\n",
    "X = data_free[features]\n",
    "y = data_free[target]\n",
    "\n",
    "# Reset the index of y to align with X\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['Game Genre', 'Developer']\n",
    "numerical_features = ['Release Month']\n",
    "\n",
    "# Preprocessing pipelines for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert the preprocessed features to a DataFrame\n",
    "encoded_cat_features = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "encoded_num_features = numerical_features\n",
    "all_features = list(encoded_cat_features) + encoded_num_features\n",
    "\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed.toarray(), columns=all_features)\n",
    "\n",
    "# Reset the index of X_preprocessed_df to align with y\n",
    "X_preprocessed_df = X_preprocessed_df.reset_index(drop=True)\n",
    "\n",
    "# Split data into training and temporary sets (80% train, 20% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_preprocessed_df, y, test_size=0.2, random_state=18\n",
    ")\n",
    "\n",
    "# Split temporary set into validation and test sets (50% each of temp -> 10% each of original)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=18\n",
    ")\n",
    "\n",
    "# Display the sizes of the splits\n",
    "print(f'Training set size: {X_train.shape[0]} samples')\n",
    "print(f'Validation set size: {X_valid.shape[0]} samples')\n",
    "print(f'Test set size: {X_test.shape[0]} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Parameter Grids for Regression Models\n",
    "import itertools\n",
    "# Parameter grids for each model\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'fit_intercept': [True, False],\n",
    "        'normalize': [True, False],\n",
    "        'alpha': [0.1, 1]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    'xgboost':{\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 1],\n",
    "        'reg_lambda': [1, 10]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': [ None, 'sqrt', 'log2']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Hàm cross-validation cho hồi quy\n",
    "def cross_validate_regression(model, X, y, k=5):\n",
    "    fold_size = len(X) // k\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    scores = {'mse': [], 'rmse': [], 'r2': []}\n",
    "    \n",
    "    for fold in range(k):\n",
    "        start = fold * fold_size\n",
    "        end = start + fold_size if fold != k-1 else len(X)\n",
    "        val_indices = indices[start:end]\n",
    "        train_indices = np.concatenate([indices[:start], indices[end:]])\n",
    "        \n",
    "        # Convert X và y thành numpy arrays nếu là pandas DataFrame hoặc Series\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "            y = y.values\n",
    "        \n",
    "        X_train_cv, y_train_cv = X[train_indices], y[train_indices]\n",
    "        X_val_cv, y_val_cv = X[val_indices], y[val_indices]\n",
    "        \n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred = model.predict(X_val_cv)\n",
    "        \n",
    "        mse = mean_squared_error(y_val_cv, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_val_cv, y_pred)\n",
    "        \n",
    "        scores['mse'].append(mse)\n",
    "        scores['rmse'].append(rmse)\n",
    "        scores['r2'].append(r2)\n",
    "        \n",
    "    avg_scores = {metric: np.mean(values) for metric, values in scores.items()}\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tất cả các kết quả tuning tham số Ridge Regression:\n",
      "fit_intercept = True, normalize = True, R² = -0.040943208198915504, MSE = 279.9311825045913\n",
      "fit_intercept = True, normalize = False, R² = 0.1715295725415565, MSE = 222.7177978116731\n",
      "fit_intercept = False, normalize = True, R² = -19.44473509642173, MSE = 5481.023646644213\n",
      "fit_intercept = False, normalize = False, R² = -0.6372386033474914, MSE = 439.9111926524962\n",
      "\n",
      "Quá trình tuning tham số Ridge Regression đã hoàn thành!\n",
      "Tham số tốt nhất: {'fit_intercept': True, 'normalize': False}\n",
      "Điểm R² tốt nhất (CV): 0.1715295725415565\n",
      "MSE tốt nhất (CV): 222.7177978116731\n",
      "\n",
      "Kết quả trên tập kiểm tra với tham số tốt nhất:\n",
      "Điểm R² trên tập kiểm tra: 0.1812880254403204\n",
      "MSE trên tập kiểm tra: 224.76617575542244\n",
      "RMSE trên tập kiểm tra: 14.992203832506496\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import itertools\n",
    "\n",
    "# Initialize variables to store the best results and all tuning outcomes for Ridge\n",
    "ridge_best_score = -np.inf\n",
    "ridge_best_mse = np.inf\n",
    "ridge_best_params = {}\n",
    "ridge_results = []\n",
    "\n",
    "# Quá trình tuning tham số Ridge Regression\n",
    "for fit_intercept in param_grids['Ridge']['fit_intercept']:\n",
    "    for normalize in [True, False]:  # Giờ sử dụng chuẩn hóa với StandardScaler\n",
    "        # Sử dụng pipeline với StandardScaler và Ridge\n",
    "        if normalize:\n",
    "            ridge_model = make_pipeline(StandardScaler(), Ridge(fit_intercept=fit_intercept))\n",
    "        else:\n",
    "            ridge_model = Ridge(fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Đánh giá mô hình với k-fold cross-validation\n",
    "        scores = cross_validate_regression(ridge_model, X_train, y_train, k=5)\n",
    "        \n",
    "        avg_r2 = scores['r2']\n",
    "        avg_mse = scores['mse']\n",
    "        \n",
    "        # Lưu kết quả vào danh sách\n",
    "        ridge_results.append({\n",
    "            'fit_intercept': fit_intercept,\n",
    "            'normalize': normalize,\n",
    "            'R2': avg_r2,\n",
    "            'MSE': avg_mse\n",
    "        })\n",
    "        \n",
    "        # Cập nhật tham số tốt nhất\n",
    "        if avg_r2 > ridge_best_score and avg_mse < ridge_best_mse:\n",
    "            ridge_best_score = avg_r2\n",
    "            ridge_best_mse = avg_mse\n",
    "            ridge_best_params = {\n",
    "                'fit_intercept': fit_intercept,\n",
    "                'normalize': normalize\n",
    "            }\n",
    "\n",
    "# In tất cả các kết quả tuning\n",
    "print(\"\\nAll Ridge parameter tuning results:\")\n",
    "for result in ridge_results:\n",
    "    print(f\"fit_intercept = {result['fit_intercept']}, normalize = {result['normalize']}, R^2 = {result['R2']}, MSE = {result['MSE']}\")\n",
    "\n",
    "# In báo cáo quá trình tuning\n",
    "print(\"\\nFine-Tuning Ridge Completed!\")\n",
    "print(f\"Best parameter: {ridge_best_params}\")\n",
    "print(f\"Best R^2: {ridge_best_score}\")\n",
    "print(f\"Best MSE: {ridge_best_mse}\")\n",
    "\n",
    "# Huấn luyện lại mô hình với tham số tốt nhất trên toàn bộ tập huấn luyện\n",
    "if ridge_best_params['normalize']:\n",
    "    ridge_best_model = make_pipeline(StandardScaler(), Ridge(fit_intercept=ridge_best_params['fit_intercept']))\n",
    "else:\n",
    "    ridge_best_model = Ridge(fit_intercept=ridge_best_params['fit_intercept'])\n",
    "\n",
    "ridge_best_model.fit(X_train, y_train)\n",
    "\n",
    "# Dự đoán trên tập kiểm tra và tính toán các chỉ số R², MSE, RMSE\n",
    "ridge_y_pred_test = ridge_best_model.predict(X_test)\n",
    "ridge_test_r2 = r2_score(y_test, ridge_y_pred_test)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_y_pred_test)\n",
    "ridge_test_rmse = np.sqrt(ridge_test_mse)\n",
    "\n",
    "# In kết quả trên tập kiểm tra\n",
    "print(\"\\nRidge Test Results:\")\n",
    "print(f\"R^2: {ridge_test_r2}\")\n",
    "print(f\"MSE: {ridge_test_mse}\")\n",
    "print(f\"RMSE: {ridge_test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize variables to store the best results and all tuning outcomes\n",
    "lasso_best_score = -np.inf\n",
    "lasso_best_mse = np.inf\n",
    "lasso_best_params = {}\n",
    "lasso_results = []\n",
    "\n",
    "# Fine-tuning Lasso Regression\n",
    "for alpha in param_grids['Lasso']['alpha']:\n",
    "    # Sử dụng pipeline với StandardScaler và Lasso\n",
    "    model = make_pipeline(StandardScaler(), Lasso(alpha=alpha, random_state=18))\n",
    "    \n",
    "    # Đánh giá mô hình với k-fold cross-validation\n",
    "    scores = cross_validate_regression(model, X_train, y_train, k=5)\n",
    "    \n",
    "    avg_r2 = scores['r2']\n",
    "    avg_mse = scores['mse']\n",
    "    \n",
    "    # Lưu kết quả vào danh sách\n",
    "    lasso_results.append({\n",
    "        'alpha': alpha,\n",
    "        'R2': avg_r2,\n",
    "        'MSE': avg_mse\n",
    "    })\n",
    "    \n",
    "    # Cập nhật tham số tốt nhất\n",
    "    if avg_r2 > lasso_best_score and avg_mse < lasso_best_mse:\n",
    "        lasso_best_score = avg_r2\n",
    "        lasso_best_mse = avg_mse\n",
    "        lasso_best_params = {'alpha': alpha}\n",
    "\n",
    "# In tất cả các kết quả tuning\n",
    "print(\"\\nAll Lasso parameter tuning results:\")\n",
    "for result in lasso_results:\n",
    "    print(f\"alpha = {result['alpha']}, R^2 = {result['R2']}, MSE = {result['MSE']}\")\n",
    "\n",
    "# In báo cáo quá trình tuning\n",
    "print(\"\\nFine-Tuning Lasso Completed!\")\n",
    "print(f\"Best parameter: {lasso_best_params}\")\n",
    "print(f\"Best R^2: {lasso_best_score}\")\n",
    "print(f\"Best MSE: {lasso_best_mse}\")\n",
    "\n",
    "# Huấn luyện lại mô hình với tham số tốt nhất trên toàn bộ tập huấn luyện\n",
    "best_lasso_model = make_pipeline(StandardScaler(), Lasso(alpha=lasso_best_params['alpha'], random_state=18))\n",
    "best_lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Dự đoán trên tập kiểm tra và tính toán các chỉ số R², MSE, RMSE\n",
    "lasso_y_pred_test = best_lasso_model.predict(X_test)\n",
    "lasso_test_r2 = r2_score(y_test, lasso_y_pred_test)\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_y_pred_test)\n",
    "lasso_test_rmse = np.sqrt(lasso_test_mse)\n",
    "\n",
    "# In kết quả trên tập kiểm tra\n",
    "print(\"\\nLasso Test Results:\")\n",
    "print(f\"R^2: {lasso_test_r2}\")\n",
    "print(f\"MSE: {lasso_test_mse}\")\n",
    "print(f\"RMSE: {lasso_test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize variables to store the best results and all tuning outcomes\n",
    "xgb_best_score = -np.inf\n",
    "xgb_best_mse = np.inf\n",
    "xgb_best_params = {}\n",
    "xgb_results = []\n",
    "\n",
    "# Fine-tuning XGBoost with parameter grid\n",
    "for n_estimators, learning_rate, max_depth, subsample, colsample_bytree, reg_alpha, reg_lambda in itertools.product(\n",
    "    param_grids['xgboost']['n_estimators'],\n",
    "    param_grids['xgboost']['learning_rate'],\n",
    "    param_grids['xgboost']['max_depth'],\n",
    "    param_grids['xgboost']['subsample'],\n",
    "    param_grids['xgboost']['colsample_bytree'],\n",
    "    param_grids['xgboost']['reg_alpha'],\n",
    "    param_grids['xgboost']['reg_lambda']\n",
    "):\n",
    "    # Create XGBoost Regressor model with the parameters\n",
    "    model = make_pipeline(\n",
    "        StandardScaler(),  # Normalize data\n",
    "        XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            objective='reg:squarederror',\n",
    "            verbosity=0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Cross-validate the model\n",
    "    scores = cross_validate_regression(model, X_train, y_train, k=5)\n",
    "    avg_r2 = scores['r2']\n",
    "    avg_mse = scores['mse']\n",
    "    \n",
    "    # Append results to the list\n",
    "    xgb_results.append({\n",
    "        'n_estimators': n_estimators,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': max_depth,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'R2': avg_r2,\n",
    "        'MSE': avg_mse\n",
    "    })\n",
    "    \n",
    "    # Update the best model parameters if current R² is higher\n",
    "    if avg_r2 > xgb_best_score and avg_mse < xgb_best_mse:\n",
    "        xgb_best_score = avg_r2\n",
    "        xgb_best_mse = avg_mse\n",
    "        xgb_best_params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'learning_rate': learning_rate,\n",
    "            'max_depth': max_depth,\n",
    "            'subsample': subsample,\n",
    "            'colsample_bytree': colsample_bytree,\n",
    "            'reg_alpha': reg_alpha,\n",
    "            'reg_lambda': reg_lambda\n",
    "        }\n",
    "\n",
    "# Print all tuning results\n",
    "print(\"\\nAll XGBoost parameter tuning results:\")\n",
    "for result in xgb_results:\n",
    "    print(f\"n_estimators = {result['n_estimators']}, learning_rate = {result['learning_rate']}, \"\n",
    "          f\"max_depth = {result['max_depth']}, subsample = {result['subsample']}, \"\n",
    "          f\"colsample_bytree = {result['colsample_bytree']}, reg_alpha = {result['reg_alpha']}, \"\n",
    "          f\"reg_lambda = {result['reg_lambda']}, R² = {result['R2']}, MSE = {result['MSE']}\")\n",
    "\n",
    "# Fine-tuning report\n",
    "print(\"\\nFine-Tuning XGBoost Completed!\")\n",
    "print(f\"Best Parameters: {xgb_best_params}\")\n",
    "print(f\"Best R²: {xgb_best_score}\")\n",
    "print(f\"Best MSE: {xgb_best_mse}\")\n",
    "\n",
    "# Train the best XGBoost model on the full training data\n",
    "best_xgb_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    XGBRegressor(\n",
    "        n_estimators=xgb_best_params['n_estimators'],\n",
    "        learning_rate=xgb_best_params['learning_rate'],\n",
    "        max_depth=xgb_best_params['max_depth'],\n",
    "        subsample=xgb_best_params['subsample'],\n",
    "        colsample_bytree=xgb_best_params['colsample_bytree'],\n",
    "        reg_alpha=xgb_best_params['reg_alpha'],\n",
    "        reg_lambda=xgb_best_params['reg_lambda'],\n",
    "        objective='reg:squarederror',\n",
    "        verbosity=0\n",
    "    )\n",
    ")\n",
    "\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and calculate R², MSE\n",
    "xgb_y_pred_test = best_xgb_model.predict(X_test)\n",
    "xgb_test_r2 = r2_score(y_test, xgb_y_pred_test)\n",
    "xgb_test_mse = mean_squared_error(y_test, xgb_y_pred_test)\n",
    "xgb_test_rmse = np.sqrt(xgb_test_mse)\n",
    "\n",
    "# Print test results\n",
    "print(\"\\nXGBoost Test Results:\")\n",
    "print(f\"R²: {xgb_test_r2}\")\n",
    "print(f\"MSE: {xgb_test_mse}\")\n",
    "print(f\"RMSE: {xgb_test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import itertools\n",
    "\n",
    "# Initialize variables to store the best results and all tuning outcomes\n",
    "dt_best_score = -np.inf\n",
    "dt_best_mse = np.inf\n",
    "dt_best_params = {}\n",
    "dt_results = []\n",
    "\n",
    "# Fine-tuning Decision Tree with parameter grid\n",
    "for max_depth, min_samples_split, min_samples_leaf, max_features in itertools.product(\n",
    "    param_grids['decision_tree']['max_depth'],\n",
    "    param_grids['decision_tree']['min_samples_split'],\n",
    "    param_grids['decision_tree']['min_samples_leaf'],\n",
    "    param_grids['decision_tree']['max_features']\n",
    "):\n",
    "    # Create Decision Tree Regressor model\n",
    "    model = make_pipeline(\n",
    "        StandardScaler(),  # Normalize data\n",
    "        DecisionTreeRegressor(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=18\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Cross-validate the model\n",
    "    scores = cross_validate_regression(model, X_train, y_train, k=5)\n",
    "    avg_r2 = scores['r2']\n",
    "    avg_mse = scores['mse']\n",
    "\n",
    "    # Append results to the list\n",
    "    dt_results.append({\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'max_features': max_features,\n",
    "        'R2': avg_r2,\n",
    "        'MSE': avg_mse\n",
    "    })\n",
    "\n",
    "    # Update the best model parameters if current R² is higher\n",
    "    if avg_r2 > dt_best_score and avg_mse < dt_best_mse:\n",
    "        dt_best_score = avg_r2\n",
    "        dt_best_mse = avg_mse\n",
    "        dt_best_params = {\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'max_features': max_features\n",
    "        }\n",
    "\n",
    "# Print all tuning results\n",
    "print(\"\\nAll Decision Tree parameter tuning results:\")\n",
    "for result in dt_results:\n",
    "    print(f\"max_depth = {result['max_depth']}, min_samples_split = {result['min_samples_split']}, \"\n",
    "          f\"min_samples_leaf = {result['min_samples_leaf']}, max_features = {result['max_features']}, \"\n",
    "          f\"R² = {result['R2']}, MSE = {result['MSE']}\")\n",
    "\n",
    "# Fine-tuning report\n",
    "print(\"\\nFine-Tuning Decision Tree Completed!\")\n",
    "print(f\"Best Parameters: {dt_best_params}\")\n",
    "print(f\"Best R²: {dt_best_score}\")\n",
    "print(f\"Best MSE: {dt_best_mse}\")\n",
    "\n",
    "# Train the best Decision Tree model on the full training data\n",
    "best_dt_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    DecisionTreeRegressor(\n",
    "        max_depth=dt_best_params['max_depth'],\n",
    "        min_samples_split=dt_best_params['min_samples_split'],\n",
    "        min_samples_leaf=dt_best_params['min_samples_leaf'],\n",
    "        max_features=dt_best_params['max_features'],\n",
    "        random_state=18\n",
    "    )\n",
    ")\n",
    "\n",
    "best_dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and calculate R², MSE\n",
    "dt_y_pred_test = best_dt_model.predict(X_test)\n",
    "dt_test_r2 = r2_score(y_test, dt_y_pred_test)\n",
    "dt_test_mse = mean_squared_error(y_test, dt_y_pred_test)\n",
    "dt_test_rmse = np.sqrt(dt_test_mse)\n",
    "\n",
    "# Print test results\n",
    "print(\"\\nDecision Tree Test Results:\")\n",
    "print(f\"R²: {dt_test_r2}\")\n",
    "print(f\"MSE: {dt_test_mse}\")\n",
    "print(f\"RMSE: {dt_test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Lưu các kết quả của 3 mô hình vào một dictionary\n",
    "results = {\n",
    "    'Model': ['Ridge', 'Lasso', 'xgboost', 'decision_tree'],\n",
    "    'R²': [ridge_test_r2, lasso_test_r2, xgb_test_r2, dt_test_r2],\n",
    "    'MSE': [ridge_test_mse, lasso_test_mse, xgb_test_mse, dt_test_mse],\n",
    "    'RMSE': [ridge_test_rmse, lasso_test_rmse, xgb_test_rmse, dt_test_rmse]\n",
    "}\n",
    "\n",
    "# Chọn mô hình tốt nhất (Dựa trên MSE hoặc R² cao nhất)\n",
    "best_model_index = np.argmin(results['MSE'])  # Chọn mô hình có MSE thấp nhất\n",
    "best_model_name = results['Model'][best_model_index]\n",
    "best_model_r2 = results['R²'][best_model_index]\n",
    "best_model_mse = results['MSE'][best_model_index]\n",
    "best_model_rmse = results['RMSE'][best_model_index]\n",
    "\n",
    "# In kết quả mô hình tốt nhất\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Model R²: {best_model_r2}\")\n",
    "print(f\"Best Model MSE: {best_model_mse}\")\n",
    "print(f\"Best Model RMSE: {best_model_rmse}\")\n",
    "\n",
    "# Vẽ biểu đồ so sánh MSE, RMSE và R² của các mô hình\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Màu sắc để làm nổi bật mô hình tốt nhất\n",
    "colors = ['lightblue' if i != best_model_index else 'lightgreen' for i in range(len(results['Model']))]\n",
    "\n",
    "# Biểu đồ MSE\n",
    "sns.barplot(x=results['Model'], y=results['MSE'], ax=ax[0], palette=colors)\n",
    "ax[0].set_title('Mean Squared Error (MSE)', fontsize=14)\n",
    "ax[0].set_xlabel('Model', fontsize=12)\n",
    "ax[0].set_ylabel('MSE', fontsize=12)\n",
    "ax[0].bar_label(ax[0].containers[0], fmt='%.2f', fontsize=10)\n",
    "\n",
    "# Biểu đồ RMSE\n",
    "sns.barplot(x=results['Model'], y=results['RMSE'], ax=ax[1], palette=colors)\n",
    "ax[1].set_title('Root Mean Squared Error (RMSE)', fontsize=14)\n",
    "ax[1].set_xlabel('Model', fontsize=12)\n",
    "ax[1].set_ylabel('RMSE', fontsize=12)\n",
    "ax[1].bar_label(ax[1].containers[0], fmt='%.2f', fontsize=10)\n",
    "\n",
    "# Biểu đồ R²\n",
    "sns.barplot(x=results['Model'], y=results['R²'], ax=ax[2], palette=colors)\n",
    "ax[2].set_title('R² Score', fontsize=14)\n",
    "ax[2].set_xlabel('Model', fontsize=12)\n",
    "ax[2].set_ylabel('R²', fontsize=12)\n",
    "ax[2].bar_label(ax[2].containers[0], fmt='%.2f', fontsize=10)\n",
    "\n",
    "# Làm nổi bật mô hình tốt nhất\n",
    "for a in ax:\n",
    "    a.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "    a.tick_params(axis='x', labelrotation=15)\n",
    "\n",
    "plt.suptitle('Comparison of Model Performance', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# In bảng kết quả để so sánh\n",
    "print(\"\\nComparison of Models:\")\n",
    "for model, mse, rmse, r2 in zip(results['Model'], results['MSE'], results['RMSE'], results['R²']):\n",
    "    print(f\"{model} - MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
