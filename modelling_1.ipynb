{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Paid Games Splits ===\n",
      "Training set size: 32808 samples\n",
      "Validation set size: 4101 samples\n",
      "Test set size: 4101 samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 1,2: Data Preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Create 'is_paid' feature: 0 for free, 1 for paid\n",
    "data['is_paid'] = data['Pricing'].apply(lambda x: 0 if x == 0.0 else 1)\n",
    "\n",
    "# Extract 'Month' from 'Release Date'\n",
    "data['Release Date'] = pd.to_datetime(data['Release Date'], errors='coerce')\n",
    "data['Release Month'] = data['Release Date'].dt.month\n",
    "\n",
    "# Filter to only paid games (is_paid == 1)\n",
    "data_paid = data[data['is_paid'] == 1].reset_index(drop=True)\n",
    "\n",
    "# Select features and target\n",
    "features = ['Game Genre', 'Developer', 'Release Month', 'Pricing']\n",
    "target = 'Rating'\n",
    "\n",
    "X = data_paid[features]\n",
    "y = data_paid[target]\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['Game Genre', 'Developer']\n",
    "numerical_features = ['Release Month', 'Pricing']\n",
    "\n",
    "# Preprocessing pipelines for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert the preprocessed features to a DataFrame\n",
    "encoded_cat_features = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "encoded_num_features = numerical_features\n",
    "all_features = list(encoded_cat_features) + encoded_num_features\n",
    "\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed.toarray(), columns=all_features)\n",
    "\n",
    "# Cell 4: Split Paid Games Data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and temporary sets (80% train, 20% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_preprocessed_df, y, test_size=0.2, random_state=18\n",
    ")\n",
    "\n",
    "# Split temporary set into validation and test sets (50% each of temp -> 10% each of original)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=18\n",
    ")\n",
    "\n",
    "# Display the sizes of the splits\n",
    "print('=== Paid Games Splits ===')\n",
    "print(f'Training set size: {X_train.shape[0]} samples')\n",
    "print(f'Validation set size: {X_valid.shape[0]} samples')\n",
    "print(f'Test set size: {X_test.shape[0]} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Parameter Grids for Regression Models\n",
    "import itertools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Parameter grids for each model\n",
    "param_grids = {\n",
    "    'LinearRegression': {\n",
    "        'fit_intercept': [True, False],\n",
    "        'normalize': [True, False]\n",
    "    },\n",
    "    'PolynomialRegression': {\n",
    "        'polynomialfeatures__degree': [2, 3, 4],\n",
    "        'polynomialfeatures__include_bias': [False],\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__normalize': [True, False]\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define Cross-Validation Function for Regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def cross_validate_regression(model, X, y, k=5):\n",
    "    fold_size = len(X) // k\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    scores = {'mse': [], 'rmse': [], 'r2': []}\n",
    "    \n",
    "    for fold in range(k):\n",
    "        start = fold * fold_size\n",
    "        end = start + fold_size if fold != k-1 else len(X)\n",
    "        val_indices = indices[start:end]\n",
    "        train_indices = np.concatenate([indices[:start], indices[end:]])\n",
    "        \n",
    "        X_train_cv, y_train_cv = X[train_indices], y[train_indices]\n",
    "        X_val_cv, y_val_cv = X[val_indices], y[val_indices]\n",
    "        \n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred = model.predict(X_val_cv)\n",
    "        \n",
    "        mse = mean_squared_error(y_val_cv, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_val_cv, y_pred)\n",
    "        \n",
    "        scores['mse'].append(mse)\n",
    "        scores['rmse'].append(rmse)\n",
    "        scores['r2'].append(r2)\n",
    "        \n",
    "    avg_scores = {metric: np.mean(values) for metric, values in scores.items()}\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([22360, 14755,  4851, 29398, 27381, 23040,  1611, 29109, 30680, 17118,\\n       ...\\n        7823,   657, 12932,  5455, 22938,  9876, 25378, 12481, 25907, 16966],\\n      dtype='int32', length=26247)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fit_intercept \u001b[38;5;129;01min\u001b[39;00m param_grids[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinearRegression\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit_intercept\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     12\u001b[0m     model \u001b[38;5;241m=\u001b[39m LinearRegression(\n\u001b[0;32m     13\u001b[0m         fit_intercept\u001b[38;5;241m=\u001b[39mfit_intercept\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# 'normalize' parameter removed\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     )\n\u001b[1;32m---> 16\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     avg_r2 \u001b[38;5;241m=\u001b[39m scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     18\u001b[0m     lr_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit_intercept\u001b[39m\u001b[38;5;124m'\u001b[39m: fit_intercept,\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_r2\n\u001b[0;32m     21\u001b[0m     })\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mcross_validate_regression\u001b[1;34m(model, X, y, k)\u001b[0m\n\u001b[0;32m     13\u001b[0m val_indices \u001b[38;5;241m=\u001b[39m indices[start:end]\n\u001b[0;32m     14\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([indices[:start], indices[end:]])\n\u001b[1;32m---> 16\u001b[0m X_train_cv, y_train_cv \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m]\u001b[49m, y[train_indices]\n\u001b[0;32m     17\u001b[0m X_val_cv, y_val_cv \u001b[38;5;241m=\u001b[39m X[val_indices], y[val_indices]\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_cv, y_train_cv)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([22360, 14755,  4851, 29398, 27381, 23040,  1611, 29109, 30680, 17118,\\n       ...\\n        7823,   657, 12932,  5455, 22938,  9876, 25378, 12481, 25907, 16966],\\n      dtype='int32', length=26247)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Cell 5: Hyperparameter Tuning for Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import itertools\n",
    "\n",
    "best_lr_score = -np.inf\n",
    "best_lr_params = {}\n",
    "lr_results = []\n",
    "\n",
    "# Updated to remove 'normalize' as it is deprecated\n",
    "for fit_intercept in param_grids['LinearRegression']['fit_intercept']:\n",
    "    model = LinearRegression(\n",
    "        fit_intercept=fit_intercept\n",
    "        # 'normalize' parameter removed\n",
    "    )\n",
    "    scores = cross_validate_regression(model, X_train, y_train, k=5)\n",
    "    avg_r2 = scores['r2']\n",
    "    lr_results.append({\n",
    "        'fit_intercept': fit_intercept,\n",
    "        'R2': avg_r2\n",
    "    })\n",
    "    if avg_r2 > best_lr_score:\n",
    "        best_lr_score = avg_r2\n",
    "        best_lr_params = {\n",
    "            'fit_intercept': fit_intercept\n",
    "        }\n",
    "\n",
    "print('Best LinearRegression Params:', best_lr_params)\n",
    "print('Best LinearRegression CV R2:', best_lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Hyperparameter Tuning for Polynomial Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "best_pr_score = -np.inf\n",
    "best_pr_params = {}\n",
    "pr_results = []\n",
    "\n",
    "for degree, include_bias, fit_intercept, normalize in itertools.product(\n",
    "    param_grids['PolynomialRegression']['polynomialfeatures__degree'],\n",
    "    param_grids['PolynomialRegression']['polynomialfeatures__include_bias'],\n",
    "    param_grids['PolynomialRegression']['linearregression__fit_intercept'],\n",
    "    param_grids['PolynomialRegression']['linearregression__normalize']\n",
    "):\n",
    "    pipeline = Pipeline([\n",
    "        ('polynomialfeatures', PolynomialFeatures(\n",
    "            degree=degree,\n",
    "            include_bias=include_bias\n",
    "        )),\n",
    "        ('linearregression', LinearRegression(\n",
    "            fit_intercept=fit_intercept,\n",
    "            normalize=normalize\n",
    "        ))\n",
    "    ])\n",
    "    scores = cross_validate_regression(pipeline, X_train, y_train, k=5)\n",
    "    avg_r2 = scores['r2']\n",
    "    pr_results.append({\n",
    "        'degree': degree,\n",
    "        'include_bias': include_bias,\n",
    "        'fit_intercept': fit_intercept,\n",
    "        'normalize': normalize,\n",
    "        'R2': avg_r2\n",
    "    })\n",
    "    if avg_r2 > best_pr_score:\n",
    "        best_pr_score = avg_r2\n",
    "        best_pr_params = {\n",
    "            'degree': degree,\n",
    "            'include_bias': include_bias,\n",
    "            'fit_intercept': fit_intercept,\n",
    "            'normalize': normalize\n",
    "        }\n",
    "\n",
    "print('Best PolynomialRegression Params:', best_pr_params)\n",
    "print('Best PolynomialRegression CV R2:', best_pr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Hyperparameter Tuning for Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "best_rf_score = -np.inf\n",
    "best_rf_params = {}\n",
    "rf_results = []\n",
    "\n",
    "for n_estimators, max_depth, min_samples_split in itertools.product(\n",
    "    param_grids['RandomForestRegressor']['n_estimators'],\n",
    "    param_grids['RandomForestRegressor']['max_depth'],\n",
    "    param_grids['RandomForestRegressor']['min_samples_split']\n",
    "):\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    scores = cross_validate_regression(model, X_train, y_train, k=5)\n",
    "    avg_r2 = scores['r2']\n",
    "    rf_results.append({\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'R2': avg_r2\n",
    "    })\n",
    "    if avg_r2 > best_rf_score:\n",
    "        best_rf_score = avg_r2\n",
    "        best_rf_params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split\n",
    "        }\n",
    "\n",
    "print('Best RandomForestRegressor Params:', best_rf_params)\n",
    "print('Best RandomForestRegressor CV R2:', best_rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Hyperparameter Tuning for MLP Regressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "best_mlp_score = -np.inf\n",
    "best_mlp_params = {}\n",
    "mlp_results = []\n",
    "\n",
    "for hidden_layer_sizes, activation, solver, alpha, learning_rate in itertools.product(\n",
    "    param_grids['MLPRegressor']['hidden_layer_sizes'],\n",
    "    param_grids['MLPRegressor']['activation'],\n",
    "    param_grids['MLPRegressor']['solver'],\n",
    "    param_grids['MLPRegressor']['alpha'],\n",
    "    param_grids['MLPRegressor']['learning_rate']\n",
    "):\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    try:\n",
    "        scores = cross_validate_regression(model, X_train, y_train, k=5)\n",
    "        avg_r2 = scores['r2']\n",
    "        mlp_results.append({\n",
    "            'hidden_layer_sizes': hidden_layer_sizes,\n",
    "            'activation': activation,\n",
    "            'solver': solver,\n",
    "            'alpha': alpha,\n",
    "            'learning_rate': learning_rate,\n",
    "            'R2': avg_r2\n",
    "        })\n",
    "        if avg_r2 > best_mlp_score:\n",
    "            best_mlp_score = avg_r2\n",
    "            best_mlp_params = {\n",
    "                'hidden_layer_sizes': hidden_layer_sizes,\n",
    "                'activation': activation,\n",
    "                'solver': solver,\n",
    "                'alpha': alpha,\n",
    "                'learning_rate': learning_rate\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f'Error with params {hidden_layer_sizes, activation, solver, alpha, learning_rate}: {e}')\n",
    "        continue\n",
    "\n",
    "print('Best MLPRegressor Params:', best_mlp_params)\n",
    "print('Best MLPRegressor CV R2:', best_mlp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train Best Models on Training Set and Evaluate on Test Set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Linear Regression\n",
    "best_lr = LinearRegression(\n",
    "    fit_intercept=best_lr_params['fit_intercept'],\n",
    "    normalize=best_lr_params['normalize']\n",
    ")\n",
    "best_lr.fit(X_train, y_train)\n",
    "lr_pred = best_lr.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_r2 = r2_score(y_test, lr_pred)\n",
    "\n",
    "print('LinearRegression Test MSE:', lr_mse)\n",
    "print('LinearRegression Test RMSE:', lr_rmse)\n",
    "print('LinearRegression Test R2:', lr_r2)\n",
    "\n",
    "# Polynomial Regression\n",
    "best_pr = Pipeline([\n",
    "    ('polynomialfeatures', PolynomialFeatures(\n",
    "        degree=best_pr_params['degree'],\n",
    "        include_bias=best_pr_params['include_bias']\n",
    "    )),\n",
    "    ('linearregression', LinearRegression(\n",
    "        fit_intercept=best_pr_params['fit_intercept'],\n",
    "        normalize=best_pr_params['normalize']\n",
    "    ))\n",
    "])\n",
    "best_pr.fit(X_train, y_train)\n",
    "pr_pred = best_pr.predict(X_test)\n",
    "pr_mse = mean_squared_error(y_test, pr_pred)\n",
    "pr_rmse = np.sqrt(pr_mse)\n",
    "pr_r2 = r2_score(y_test, pr_pred)\n",
    "\n",
    "print('PolynomialRegression Test MSE:', pr_mse)\n",
    "print('PolynomialRegression Test RMSE:', pr_rmse)\n",
    "print('PolynomialRegression Test R2:', pr_r2)\n",
    "\n",
    "# Random Forest Regressor\n",
    "best_rf = RandomForestRegressor(\n",
    "    n_estimators=best_rf_params['n_estimators'],\n",
    "    max_depth=best_rf_params['max_depth'],\n",
    "    min_samples_split=best_rf_params['min_samples_split'],\n",
    "    random_state=42\n",
    ")\n",
    "best_rf.fit(X_train, y_train)\n",
    "rf_pred = best_rf.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "\n",
    "print('RandomForestRegressor Test MSE:', rf_mse)\n",
    "print('RandomForestRegressor Test RMSE:', rf_rmse)\n",
    "print('RandomForestRegressor Test R2:', rf_r2)\n",
    "\n",
    "# MLP Regressor\n",
    "best_mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=best_mlp_params['hidden_layer_sizes'],\n",
    "    activation=best_mlp_params['activation'],\n",
    "    solver=best_mlp_params['solver'],\n",
    "    alpha=best_mlp_params['alpha'],\n",
    "    learning_rate=best_mlp_params['learning_rate'],\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "best_mlp.fit(X_train, y_train)\n",
    "mlp_pred = best_mlp.predict(X_test)\n",
    "mlp_mse = mean_squared_error(y_test, mlp_pred)\n",
    "mlp_rmse = np.sqrt(mlp_mse)\n",
    "mlp_r2 = r2_score(y_test, mlp_pred)\n",
    "\n",
    "print('MLPRegressor Test MSE:', mlp_mse)\n",
    "print('MLPRegressor Test RMSE:', mlp_rmse)\n",
    "print('MLPRegressor Test R2:', mlp_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Compare Model Performances\n",
    "import pandas as pd\n",
    "\n",
    "performance = pd.DataFrame({\n",
    "    'Model': ['LinearRegression', 'PolynomialRegression', 'RandomForestRegressor', 'MLPRegressor'],\n",
    "    'MSE': [lr_mse, pr_mse, rf_mse, mlp_mse],\n",
    "    'RMSE': [lr_rmse, pr_rmse, rf_rmse, mlp_rmse],\n",
    "    'R2': [lr_r2, pr_r2, rf_r2, mlp_r2]\n",
    "})\n",
    "\n",
    "print('Model Comparison:')\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Report Fine-Tuning Process and Model Performances\n",
    "\n",
    "print(\"=== Hyperparameter Tuning Results ===\\n\")\n",
    "\n",
    "print(\"1. **Linear Regression**\")\n",
    "print(f\"   - Best Parameters: {best_lr_params}\")\n",
    "print(f\"   - Best CV R² Score: {best_lr_score}\\n\")\n",
    "\n",
    "print(\"2. **Polynomial Regression**\")\n",
    "print(f\"   - Best Parameters: {best_pr_params}\")\n",
    "print(f\"   - Best CV R² Score: {best_pr_score}\\n\")\n",
    "\n",
    "print(\"3. **Random Forest Regressor**\")\n",
    "print(f\"   - Best Parameters: {best_rf_params}\")\n",
    "print(f\"   - Best CV R² Score: {best_rf_score}\\n\")\n",
    "\n",
    "print(\"4. **MLP Regressor**\")\n",
    "print(f\"   - Best Parameters: {best_mlp_params}\")\n",
    "print(f\"   - Best CV R² Score: {best_mlp_score}\\n\")\n",
    "\n",
    "print(\"=== Model Performance on Test Set ===\\n\")\n",
    "\n",
    "performance = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Polynomial Regression', 'Random Forest Regressor', 'MLP Regressor'],\n",
    "    'MSE': [lr_mse, pr_mse, rf_mse, mlp_mse],\n",
    "    'RMSE': [lr_rmse, pr_rmse, rf_rmse, mlp_rmse],\n",
    "    'R²': [lr_r2, pr_r2, rf_r2, mlp_r2]\n",
    "})\n",
    "\n",
    "print(performance.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
